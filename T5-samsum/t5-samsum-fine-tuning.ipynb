{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sumeval\n!pip install py7zr","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:04.007872Z","iopub.execute_input":"2022-11-15T18:24:04.008754Z","iopub.status.idle":"2022-11-15T18:24:38.732754Z","shell.execute_reply.started":"2022-11-15T18:24:04.008642Z","shell.execute_reply":"2022-11-15T18:24:38.731614Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sumeval\n  Downloading sumeval-0.2.2.tar.gz (80 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m896.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting plac>=0.9.6\n  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\nCollecting sacrebleu>=1.3.2\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.3.2->sumeval) (0.8.10)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.3.2->sumeval) (4.9.1)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.3.2->sumeval) (2.5.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.3.2->sumeval) (1.21.6)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.3.2->sumeval) (0.4.5)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.3.2->sumeval) (2021.11.10)\nBuilding wheels for collected packages: sumeval\n  Building wheel for sumeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sumeval: filename=sumeval-0.2.2-py3-none-any.whl size=54549 sha256=7b57bc3c9a25bf986ff8e404272355e6b78260efb651e829a8512bbd5ed42430\n  Stored in directory: /root/.cache/pip/wheels/f4/3f/31/c521bdfba2be7518bd94ba3e8b982812822167cc0497fad192\nSuccessfully built sumeval\nInstalling collected packages: plac, sacrebleu, sumeval\nSuccessfully installed plac-1.3.5 sacrebleu-2.3.1 sumeval-0.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting py7zr\n  Downloading py7zr-0.20.2-py3-none-any.whl (65 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m737.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pyzstd>=0.14.4\n  Downloading pyzstd-0.15.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (379 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m379.2/379.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1\n  Downloading pyppmd-1.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m138.6/138.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pycryptodomex>=3.6.6\n  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from py7zr) (4.12.0)\nCollecting brotli>=1.0.9\n  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multivolumefile>=0.2.3\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting inflate64>=0.3.1\n  Downloading inflate64-0.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m93.6/93.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from py7zr) (5.9.1)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.7/site-packages (from py7zr) (1.6.4)\nCollecting pybcj>=0.6.0\n  Downloading pybcj-1.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->py7zr) (4.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->py7zr) (3.8.0)\nInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, multivolumefile, pybcj, inflate64, py7zr\nSuccessfully installed brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.2 pybcj-1.0.1 pycryptodomex-3.15.0 pyppmd-1.0.0 pyzstd-0.15.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Decription \n### Fine-Tune a T5 model on the samsum dataset using Pytorch and HugingFace.","metadata":{}},{"cell_type":"code","source":"import gc\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sumeval.metrics.rouge import RougeCalculator\n\nimport torch\nfrom transformers import T5Tokenizer\nimport transformers\nfrom transformers import T5ForConditionalGeneration\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport os\nfrom datasets import load_dataset, Dataset\nfrom transformers import get_scheduler\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\n\nprint('Pytorch version: %s'  % torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:38.736231Z","iopub.execute_input":"2022-11-15T18:24:38.736799Z","iopub.status.idle":"2022-11-15T18:24:45.217278Z","shell.execute_reply.started":"2022-11-15T18:24:38.736718Z","shell.execute_reply":"2022-11-15T18:24:45.216205Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Pytorch version: 1.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"warnings.simplefilter('ignore')\npd.set_option('display.max_colwidth', 100)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:45.219033Z","iopub.execute_input":"2022-11-15T18:24:45.220040Z","iopub.status.idle":"2022-11-15T18:24:45.224878Z","shell.execute_reply.started":"2022-11-15T18:24:45.219983Z","shell.execute_reply":"2022-11-15T18:24:45.223930Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data","metadata":{"execution":{"iopub.status.busy":"2022-10-02T13:28:32.324877Z","iopub.execute_input":"2022-10-02T13:28:32.325478Z","iopub.status.idle":"2022-10-02T13:28:32.351848Z","shell.execute_reply.started":"2022-10-02T13:28:32.325361Z","shell.execute_reply":"2022-10-02T13:28:32.350830Z"}}},{"cell_type":"code","source":"dataset = load_dataset(\"samsum\")","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:45.227584Z","iopub.execute_input":"2022-11-15T18:24:45.227965Z","iopub.status.idle":"2022-11-15T18:24:52.383869Z","shell.execute_reply.started":"2022-11-15T18:24:45.227911Z","shell.execute_reply":"2022-11-15T18:24:52.382885Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b06b34d78d45cead592949a746e7e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/770 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54d814bf46847fc8ec7af141aba12ba"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c33a2f7b00f42039d0b8fd5907df392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"783ac84c3aad445089335b916fbafad9"}},"metadata":{}}]},{"cell_type":"code","source":"def add_prefix(train, valid, test):\n    # adding summarize in front of data\n    train_df = pd.DataFrame(train)\n    train_df[\"dialogue\"] = \"summarize: \" + train_df[\"dialogue\"]\n    train = Dataset.from_pandas(train_df)\n\n    valid_df = pd.DataFrame(valid)\n    valid_df[\"dialogue\"] = \"summarize: \" + valid_df[\"dialogue\"]\n    valid = Dataset.from_pandas(valid_df)\n\n    test_df = pd.DataFrame(test)\n    test_df[\"dialogue\"] = \"summarize: \" + test_df[\"dialogue\"]\n    test = Dataset.from_pandas(test_df)\n\n    return train, valid, test\n\ndef random_subset(train, frac):\n    train_df = pd.DataFrame(train)\n    train_df = train_df.sample(frac = frac, random_state = 2) \n    return Dataset.from_pandas(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:52.385395Z","iopub.execute_input":"2022-11-15T18:24:52.385987Z","iopub.status.idle":"2022-11-15T18:24:52.394247Z","shell.execute_reply.started":"2022-11-15T18:24:52.385949Z","shell.execute_reply":"2022-11-15T18:24:52.393308Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train = dataset[\"train\"]\nvalid = dataset[\"validation\"]\ntest = dataset[\"test\"]\n\ntrain = train.remove_columns([\"id\"])\nvalid = valid.remove_columns([\"id\"])\ntest = test.remove_columns([\"id\"])\n\ntrain, valid, test = add_prefix(train, valid, test)\n\nprint(len(train), len(valid), len(test))\nprint(\"dataset has features: \", train)\nprint(\"sample input and output is\")\nprint(train[0][\"dialogue\"])\nprint(train[0][\"summary\"])\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:52.395693Z","iopub.execute_input":"2022-11-15T18:24:52.396268Z","iopub.status.idle":"2022-11-15T18:24:53.308340Z","shell.execute_reply.started":"2022-11-15T18:24:52.396233Z","shell.execute_reply":"2022-11-15T18:24:53.307302Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"14732 818 819\ndataset has features:  Dataset({\n    features: ['dialogue', 'summary'],\n    num_rows: 14732\n})\nsample input and output is\nsummarize: Amanda: I baked  cookies. Do you want some?\nJerry: Sure!\nAmanda: I'll bring you tomorrow :-)\nAmanda baked cookies and will bring Jerry some tomorrow.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Taking 30% of train data for training\ntrain = random_subset(train, 1)\nprint(len(train))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:53.309673Z","iopub.execute_input":"2022-11-15T18:24:53.310351Z","iopub.status.idle":"2022-11-15T18:24:54.102205Z","shell.execute_reply.started":"2022-11-15T18:24:53.310312Z","shell.execute_reply":"2022-11-15T18:24:54.101149Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"14732\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Building custom Data class","metadata":{}},{"cell_type":"code","source":"# this class object will be passed to the dataloader\nclass LoadData(torch.utils.data.Dataset):\n    \"\"\"\n    Using this since dataloader expects map-style dataset objects\n    \n    \"\"\"\n    \n    def __init__(\n        self, dataset, tokenizer, source_length, target_length):\n        \"\"\"\n        Initializes a Dataset class\n\n        Args:\n            dataset (Dataset object): Input Dataset\n            tokenizer (Tokenizer object): Transformer tokenizer\n            source_length (int): Max length of source text\n            target_length (int): Max length of target text\n        \"\"\"\n        \n        self.tokenizer = tokenizer\n        self.data = dataset\n        self.source_length = source_length\n        self.summary_length = target_length\n        self.target_text = self.data[\"summary\"]\n        self.source_text = self.data[\"dialogue\"]\n\n    def __len__(self):\n        return len(self.target_text)\n\n    def __getitem__(self, index):\n        \"\"\"\n        return input ids, attention masks and target ids\n        \n        \"\"\"\n        source_text = str(self.source_text[index])\n        target_text = str(self.target_text[index])\n\n        # cleaning data so as to ensure data is in string type\n        source_text = \" \".join(source_text.split())\n        target_text = \" \".join(target_text.split())\n\n        source = self.tokenizer.__call__(\n            [source_text],\n            max_length=self.source_length,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        \n        target = self.tokenizer.__call__(\n            [target_text],\n            max_length=self.summary_length,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n\n        source_ids = source[\"input_ids\"].squeeze()\n        source_mask = source[\"attention_mask\"].squeeze()\n        target_ids = target[\"input_ids\"].squeeze()\n        target_mask = target[\"attention_mask\"].squeeze()\n\n        return {\n            \"source_ids\": source_ids.to(dtype=torch.long),\n            \"source_mask\": source_mask.to(dtype=torch.long),\n            \"target_ids\": target_ids.to(dtype=torch.long),\n            \"target_ids_y\": target_ids.to(dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:54.104594Z","iopub.execute_input":"2022-11-15T18:24:54.105321Z","iopub.status.idle":"2022-11-15T18:24:54.116325Z","shell.execute_reply.started":"2022-11-15T18:24:54.105269Z","shell.execute_reply":"2022-11-15T18:24:54.115325Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"rouge = RougeCalculator(stopwords=True, lang=\"en\")\n\ndef rouge_calc(preds, targets):\n    rouge_1 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=1) for i in range(len(preds))]\n    rouge_2 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=2) for i in range(len(preds))]\n    rouge_l = [rouge.rouge_l(summary=preds[i],references=targets[i]) for i in range(len(preds))]\n\n    return {\"Rouge_1\": np.array(rouge_1).mean(),\n            \"Rouge_2\": np.array(rouge_2).mean(),\n            \"Rouge_L\": np.array(rouge_l).mean()}\n\ndef evaluate(model, eval_dataloader, tokenizer, device):\n    predictions = []\n    ground_truths = []\n    losses = []\n    \n    with torch.no_grad():\n        for eval_batch in eval_dataloader:\n            y = eval_batch['target_ids'].to(device, dtype = torch.long)\n            ids = eval_batch['source_ids'].to(device, dtype = torch.long)\n            mask = eval_batch['source_mask'].to(device, dtype = torch.long)\n            \n            #print(type(ids), np.shape(ids))\n\n            generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150, \n              #num_beams=2,\n              repetition_penalty=2.5,  # there is a research paper for this\n              #length_penalty=1.0,  # > 0 encourages to generate short sentences, < 0 to generate long sentences\n              #early_stopping=True  # stops beam search when number of beams sentences are generated per batch\n              )\n            \n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n            \n            predictions += preds\n            ground_truths += target\n            \n            y_ids = y[:, :-1].contiguous()\n            lm_labels = y[:, 1:].clone().detach()\n            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n\n            loss = model(\n                input_ids=ids,\n                attention_mask=mask,\n                decoder_input_ids=y_ids,\n                labels=lm_labels,\n            )[0]\n            losses.append(loss.item())\n\n    scores = rouge_calc(predictions, ground_truths)\n    avg_loss = sum(losses)/len(losses)\n    print(\"Validation data score and losses are\", scores, avg_loss)\n    return avg_loss\n\ndef train_(model, train_loader, valid_loader, device, tokenizer, optimizer):\n    steps = 0\n    last_loss = 1000\n    \n    checkpoint_path = parameters[\"out_dir\"] + \"best_checkpoint/\"\n    if not os.path.exists(checkpoint_path):\n        os.makedirs(checkpoint_path)\n    \n    for epoch in range(parameters[\"epochs\"]):\n        print(\"Epoch: \", epoch)    \n        for batch in train_loader:\n            model.train()\n            y = batch[\"target_ids\"].to(device, dtype=torch.long)\n\n            y_ids = y[:, :-1].contiguous()  # inputs passed to decoder, start token for decoder is pad token\n            lm_labels = y[:, 1:].clone().detach()  # since it is lm task\n            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100  # so that loss can ignore the padded tokens\n\n            ids = batch[\"source_ids\"].to(device, dtype=torch.long)\n            mask = batch[\"source_mask\"].to(device, dtype=torch.long)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                decoder_input_ids=y_ids,\n                labels=lm_labels,\n            )\n            loss = outputs[0]\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if steps % 400 == 0: print(\"Train loss on {}th step is {}\".format(steps, loss.item()))\n            \"\"\"\n            if steps % 400 == 0:\n                model.eval()\n                print(\"Train loss on {}th step is {}\".format(steps, loss.item()))\n                loss = evaluate(model, valid_loader, tokenizer, device)\n                if loss < last_loss: # save model parameters\n                    model.save_pretrained(checkpoint_path)\n                    tokenizer.save_pretrained(checkpoint_path)\n                    torch.save(optimizer.state_dict(), os.path.join(checkpoint_path, \"optimizer.pt\"))\n                    last_loss = loss\"\"\"\n            steps += 1\n    \n    \"\"\"loss = evaluate(model, valid_loader, tokenizer)\n    if loss < last_loss: # save model parameters\n        model.save_pretrained(checkpoint_path)\n        tokenizer.save_pretrained(checkpoint_path)\n        torch.save(optimizer.state_dict(), os.path.join(checkpoint_path, \"optimizer.pt\"))\n        last_loss = loss\"\"\"\n    \n    # save the last model weights\n    model.save_pretrained(parameters[\"out_dir\"])\n    tokenizer.save_pretrained(parameters[\"out_dir\"])\n    torch.save(optimizer.state_dict(), os.path.join(parameters[\"out_dir\"], \"optimizer.pt\"))\n\ndef train_model(parameters, train_dataset, valid_dataset):\n    cuda =  torch.cuda.is_available()\n    device = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n    \n    tokenizer = T5Tokenizer.from_pretrained(parameters[\"model\"])    \n    model = T5ForConditionalGeneration.from_pretrained(parameters[\"model\"])  # has 60M parameters\n    model = model.to(device)\n    \n    optimizer = AdamW(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"wd\"])\n    \n    train_obj = LoadData(\n        train_dataset,\n        tokenizer,\n        parameters[\"max_source_length\"],\n        parameters[\"max_target_length\"]\n    )\n    \n    val_obj = LoadData(\n        valid_dataset,\n        tokenizer,\n        parameters[\"max_source_length\"],\n        parameters[\"max_target_length\"]\n    )\n    \n    train_loader = DataLoader(train_obj, shuffle=True, batch_size=parameters[\"train_bs\"])\n    valid_loader = DataLoader(val_obj, shuffle=False, batch_size=parameters[\"val_bs\"])\n    \n    num_training_steps = parameters[\"epochs\"] * len(train_loader)\n    print(\"Training steps are\", num_training_steps)\n\n    train_(model, train_loader, valid_loader, device, tokenizer, optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:24:54.117926Z","iopub.execute_input":"2022-11-15T18:24:54.118558Z","iopub.status.idle":"2022-11-15T18:24:54.144974Z","shell.execute_reply.started":"2022-11-15T18:24:54.118520Z","shell.execute_reply":"2022-11-15T18:24:54.144032Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"parameters = {\"model\": \"t5-small\",  # model_type: t5-base/t5-large\n    \"train_bs\": 8,  # training batch size\n    \"val_bs\": 8,  # validation batch size\n    \"epochs\": 5,  # number of training epochs\n    \"lr\": 1e-4,  # learning rate\n    \"wd\": 0.01,  # learning rate\n    \"max_source_length\": 512,  # max length of source text\n    \"max_target_length\": 80,  # max length of target text\n    \"SEED\": 42,\n    \"out_dir\": \"./T5-samsum-alltrain/\"}","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:44:29.563968Z","iopub.execute_input":"2022-11-15T18:44:29.564349Z","iopub.status.idle":"2022-11-15T18:44:29.569373Z","shell.execute_reply.started":"2022-11-15T18:44:29.564319Z","shell.execute_reply":"2022-11-15T18:44:29.568412Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_model(parameters, train, valid)","metadata":{"execution":{"iopub.status.busy":"2022-10-15T15:27:42.388577Z","iopub.execute_input":"2022-10-15T15:27:42.388941Z","iopub.status.idle":"2022-10-15T15:55:28.062731Z","shell.execute_reply.started":"2022-10-15T15:27:42.388909Z","shell.execute_reply":"2022-10-15T15:55:28.061283Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training steps are 8290\nEpoch:  0\nTrain loss on 0th step is 4.062132835388184\nTrain loss on 400th step is 1.9140644073486328\nTrain loss on 800th step is 1.9817224740982056\nTrain loss on 1200th step is 2.3413779735565186\nTrain loss on 1600th step is 1.4699466228485107\nEpoch:  1\nTrain loss on 2000th step is 1.9198647737503052\nTrain loss on 2400th step is 1.8585165739059448\nTrain loss on 2800th step is 1.9449955224990845\nTrain loss on 3200th step is 1.4649343490600586\nEpoch:  2\nTrain loss on 3600th step is 1.997383713722229\nTrain loss on 4000th step is 1.1843547821044922\nTrain loss on 4400th step is 2.2455718517303467\nTrain loss on 4800th step is 1.4643442630767822\nEpoch:  3\nTrain loss on 5200th step is 1.4191169738769531\nTrain loss on 5600th step is 1.9597364664077759\nTrain loss on 6000th step is 1.8833582401275635\nTrain loss on 6400th step is 1.117913842201233\nEpoch:  4\nTrain loss on 6800th step is 1.7629506587982178\nTrain loss on 7200th step is 2.04990291595459\nTrain loss on 7600th step is 2.1069469451904297\nTrain loss on 8000th step is 1.7987552881240845\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Make some predictions","metadata":{}},{"cell_type":"code","source":"cuda =  torch.cuda.is_available()\ndevice = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n\ntokenizer = T5Tokenizer.from_pretrained(parameters[\"out_dir\"], do_lower_case=False)\nmodel = T5ForConditionalGeneration.from_pretrained(parameters[\"out_dir\"])\nmodel.to(device)\n\nval_obj = LoadData(\n        valid,\n        tokenizer,\n        parameters[\"max_source_length\"],\n        parameters[\"max_target_length\"]\n    )\nvalid_loader = DataLoader(val_obj, shuffle=False, batch_size=parameters[\"val_bs\"])\n\ntest_obj = LoadData(\n        test,\n        tokenizer,\n        parameters[\"max_source_length\"],\n        parameters[\"max_target_length\"]\n    )\n\ntest_loader = DataLoader(test_obj, shuffle=False, batch_size=parameters[\"val_bs\"])\n\nevaluate(model, valid_loader, tokenizer, device)\nevaluate(model, test_loader, tokenizer, device)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:44:36.015715Z","iopub.execute_input":"2022-11-15T18:44:36.016187Z","iopub.status.idle":"2022-11-15T18:47:58.105487Z","shell.execute_reply.started":"2022-11-15T18:44:36.016150Z","shell.execute_reply":"2022-11-15T18:47:58.104435Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Validation data score and losses are {'Rouge_1': 0.5005497255067427, 'Rouge_2': 0.18086332729221619, 'Rouge_L': 0.41342872683905424} 1.701273875329101\nValidation data score and losses are {'Rouge_1': 0.49109948828069394, 'Rouge_2': 0.17406329182965535, 'Rouge_L': 0.4128331616732573} 1.7074786725553495\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1.7074786725553495"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(r'pytorch_model.bin')","metadata":{"execution":{"iopub.status.busy":"2022-10-15T17:02:14.234489Z","iopub.execute_input":"2022-10-15T17:02:14.235027Z","iopub.status.idle":"2022-10-15T17:02:14.251915Z","shell.execute_reply.started":"2022-10-15T17:02:14.234980Z","shell.execute_reply":"2022-10-15T17:02:14.250880Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/pytorch_model.bin","text/html":"<a href='pytorch_model.bin' target='_blank'>pytorch_model.bin</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# load the pre-trained best-checkpoint model\ndef inference(model, device, tokenizer, parameters):\n    dataset = load_dataset(\"samsum\")\n    test = dataset[\"test\"]\n    test = test.remove_columns([\"id\"])\n    \n    sent = test[0][\"dialogue\"]  # taking an example sentence\n    sent = \" \".join(sent.split())\n    \n    source = tokenizer.__call__(\n            [sent],\n            max_length=parameters[\"max_source_length\"],\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n    \n    ids = source[\"input_ids\"]\n    mask = source[\"attention_mask\"]\n    \n    model.eval()\n    with torch.no_grad():\n        ids = ids.to(device, dtype = torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        \n        generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150, \n              #num_beams=2,\n              repetition_penalty=2.5,  # there is a research paper for this\n              #length_penalty=1.0,  # > 0 encourages to generate short sentences, < 0 to generate long sentences\n              #early_stopping=True  # stops beam search when number of beams sentences are generated per batch\n              )\n        \n        preds = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        \n        print(\"Input dialogue is: \", sent)\n        print(\"Output summary is: \", preds)\n    \n    \n    \ncuda =  torch.cuda.is_available()\ndevice = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n\ntokenizer = T5Tokenizer.from_pretrained(parameters[\"out_dir\"], do_lower_case=False)\nmodel = T5ForConditionalGeneration.from_pretrained(parameters[\"out_dir\"])\nmodel.to(device)\ninference(model, device, tokenizer, parameters)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:47:58.107603Z","iopub.execute_input":"2022-11-15T18:47:58.107979Z","iopub.status.idle":"2022-11-15T18:48:00.291023Z","shell.execute_reply.started":"2022-11-15T18:47:58.107941Z","shell.execute_reply":"2022-11-15T18:48:00.289966Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f29ea672444a4891912e689c8a94de85"}},"metadata":{}},{"name":"stdout","text":"Input dialogue is:  Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 泗 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\nOutput summary is:  Hannah has Betty's number. Amanda can't find it. Larry called her last time they were at the park together.\n","output_type":"stream"}]},{"cell_type":"code","source":"# save the results in a file\noutputs = {\"source_text\": [], \"prediction\": [], \"label\": [], \"split\": []}\n\nmodel.eval()\n\nwith torch.no_grad():\n    for eval_batch in valid_loader:\n        y = eval_batch['target_ids'].to(device, dtype = torch.long)\n        ids = eval_batch['source_ids'].to(device, dtype = torch.long)\n        mask = eval_batch['source_mask'].to(device, dtype = torch.long)\n\n        generated_ids = model.generate(\n          input_ids = ids,\n          attention_mask = mask, \n          max_length=150, \n          #num_beams=2,\n          repetition_penalty=2.5,  # there is a research paper for this\n          #length_penalty=1.0,  # > 0 encourages to generate short sentences, < 0 to generate long sentences\n          #early_stopping=True  # stops beam search when number of beams sentences are generated per batch\n          )\n\n        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n        sources = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in ids]\n        target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n        \n        for i,out in enumerate(preds):\n            outputs[\"prediction\"].append(out)\n            outputs[\"label\"].append(target[i])\n            outputs[\"source_text\"].append(sources[i])\n            outputs[\"split\"].append(\"valid\")\n    \n    for test_batch in test_loader:\n        y = test_batch['target_ids'].to(device, dtype = torch.long)\n        ids = test_batch['source_ids'].to(device, dtype = torch.long)\n        mask = test_batch['source_mask'].to(device, dtype = torch.long)\n\n        generated_ids = model.generate(\n          input_ids = ids,\n          attention_mask = mask, \n          max_length=150, \n          #num_beams=2,\n          repetition_penalty=2.5,  # there is a research paper for this\n          #length_penalty=1.0,  # > 0 encourages to generate short sentences, < 0 to generate long sentences\n          #early_stopping=True  # stops beam search when number of beams sentences are generated per batch\n          )\n\n        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n        sources = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in ids]\n        target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n        \n        for i,out in enumerate(preds):\n            outputs[\"prediction\"].append(out)\n            outputs[\"label\"].append(target[i])\n            outputs[\"source_text\"].append(sources[i])\n            outputs[\"split\"].append(\"test\")\n\n\noutputs = pd.DataFrame.from_dict(outputs)\noutputs.to_csv(\"./T5_samsumall.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-15T18:48:00.294100Z","iopub.execute_input":"2022-11-15T18:48:00.294372Z","iopub.status.idle":"2022-11-15T18:56:39.817564Z","shell.execute_reply.started":"2022-11-15T18:48:00.294346Z","shell.execute_reply":"2022-11-15T18:56:39.816562Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}