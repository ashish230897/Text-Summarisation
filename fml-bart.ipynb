{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentencepiece\n!pip install transformers\n!pip install torch\n!pip install rich[jupyter]\n!pip install -q sumeval==0.2.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sumeval.metrics.rouge import RougeCalculator\nimport torch\nimport transformers\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\nprint('Pytorch version: %s'  % torch.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ntrain= load_dataset('multi_news', split='train[:1%]')\ntest= load_dataset('multi_news', split='test[:1%]')\nprint(len(train),len(test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata = {'text': train['document'],\n        'summary': train['summary']}\ndf = pd.DataFrame(data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n# , RandomSampler, SequentialSampler\nimport os\n\nfrom rich.table import Column, Table\nfrom rich import box\nfrom rich.console import Console\n\n# define a rich console logger\nconsole=Console(record=True)\n\ndef display_df(df):\n  \"\"\"display dataframe in ASCII format\"\"\"\n\n  console=Console()\n  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n\n  for i, row in enumerate(df.values.tolist()):\n    table.add_row(row[0], row[1])\n\n  console.print(table)\n\ntraining_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n                        Column(\"Steps\", justify=\"center\"),\n                        Column(\"Loss\", justify=\"center\"), \n                        title=\"Training Status\",pad_edge=False, box=box.ASCII)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the device for GPU usage\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformMultiNews(Dataset):\n  \"\"\"\n  Creating a custom dataset for reading the dataset and \n  loading it into the dataloader to pass it to the neural network for finetuning the model\n\n  \"\"\"\n\n  def __init__(self, dataframe, tokenizer, sourcelen, targetlen, source_data, target_data):\n    self.tokenizer = tokenizer\n    self.data = dataframe\n    self.sourcelen = sourcelen\n    self.summ_len = targetlen\n    self.target_data = self.data[target_data]\n    self.source_data = self.data[source_data]\n\n  def __len__(self):\n    return len(self.target_data)\n\n  def __getitem__(self, index):\n    source_data = str(self.source_data[index])\n    target_data = str(self.target_data[index])\n\n    #cleaning data so as to ensure data is in string type\n    source_data = ' '.join(source_data.split())\n    target_data = ' '.join(target_data.split())\n\n    source = self.tokenizer.batch_encode_plus([source_data], max_length= self.sourcelen, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n    target = self.tokenizer.batch_encode_plus([target_data], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n\n    sourceids = source['input_ids'].squeeze()\n    sourcemask = source['attention_mask'].squeeze()\n    targetids = target['input_ids'].squeeze()\n    targetmask = target['attention_mask'].squeeze()\n\n    return {\n        'source_ids': sourceids.to(dtype=torch.long), \n        'source_mask': sourcemask.to(dtype=torch.long), \n        'target_ids': targetids.to(dtype=torch.long),\n        'target_ids_y': targetids.to(dtype=torch.long)\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch, tokenizer, model, device, loader, optimizer):\n\n    #Function to train model \n\n  model.train()\n  for _,data in enumerate(loader, 0):\n    y = data['target_ids'].to(device, dtype = torch.long)\n    y_ids = y[:, :-1].contiguous()\n    labellm = y[:, 1:].clone().detach()\n    labellm[y[:, 1:] == tokenizer.pad_token_id] = -100\n    ids = data['source_ids'].to(device, dtype = torch.long)\n    mask = data['source_mask'].to(device, dtype = torch.long)\n\n    outputm = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=labellm)\n    loss = outputm[0]\n\n    if _%10==0:\n      training_logger.add_row(str(epoch), str(_), str(loss))\n      console.print(training_logger)\n      torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, './outputs/training_model')\n      \n        \n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n\n  \n  #Function to evaluate model \n\n \n  model.eval()\n  predictiondata = []\n  actualdata = []\n  with torch.no_grad():\n      for _, data in enumerate(loader, 0):\n          y = data['target_ids'].to(device, dtype = torch.long)\n          ids = data['source_ids'].to(device, dtype = torch.long)\n          mask = data['source_mask'].to(device, dtype = torch.long)\n\n          generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150, \n              repetition_penalty=2.5, \n              )\n          predicted = [tokenizer.decode(g_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g_id in generated_ids]\n          target = [tokenizer.decode(t_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t_id in y]\n          if _%10==0:\n              console.print(f'Completed {_}')\n\n          predictiondata.extend(predicted)\n          actualdata.extend(target)\n  return predictiondata, actualdata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BartTrainer(dataframe, source_data, target_data, model_params, output_dir=\"./outputs/\" ):\n  \n  \"\"\"\n  Bart trainer\n\n  \"\"\"\n\n  # Set random seeds and deterministic pytorch for reproducibility\n  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n  torch.backends.cudnn.deterministic = True \n  \n  # tokenzier for encoding the text\n  \n  tokenizer = BartTokenizer.from_pretrained(model_params[\"MODEL\"])\n  model = BartForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])   \n  model = model.to(device)\n  \n  # Importing the raw dataset\n  dataframe = dataframe[[source_data,target_data]]\n  \n  train_size = 0.8\n  train_ds=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n  valid_ds=dataframe.drop(train_ds.index).reset_index(drop=True)\n  train_ds = train_ds.reset_index(drop=True)\n\n\n  # Creating the Training and Validation dataset for further creation of Dataloader\n  training_set = TransformMultiNews(train_ds, tokenizer, model_params[\"MAX_LENGTH_SOURCE_TEXT\"], model_params[\"MAX_LENGTH_TARGET_TEXT\"], source_data, target_data)\n  valid_set = TransformMultiNews(valid_ds, tokenizer, model_params[\"MAX_LENGTH_SOURCE_TEXT\"], model_params[\"MAX_LENGTH_TARGET_TEXT\"], source_data, target_data)\n\n\n  # Defining the parameters for creation of dataloaders\n  train_params = {\n      'batch_size': model_params[\"BATCH_SIZE_TRAIN\"],\n      'shuffle': True,\n      'num_workers': 0\n      }\n\n\n  valid_params = {\n      'batch_size': model_params[\"BATCH_SIZE_VALID\"],\n      'shuffle': False,\n      'num_workers': 0\n      }\n\n\n  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n  train_loader = DataLoader(training_set, **train_params)\n  val_loader = DataLoader(valid_set, **valid_params)\n\n\n  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n\n    \n\n\n  # Training loop\n\n  console.log(f'[Initiating Fine Tuning]...\\n')\n \n\n  for epoch1 in range(model_params[\"TRAIN_EPOCHS\"]):\n      train(epoch1, tokenizer, model, device, train_loader, optimizer)\n        \n# Training loop ends\n      \n  console.log(f\"[Saving Model]...\\n\")\n  #Saving the model after training\n  path = os.path.join(output_dir, \"model_files\")\n  model.save_pretrained(path)\n  tokenizer.save_pretrained(path)\n\n\n  # evaluating test dataset\n  console.log(f\"[Initiating Validation]...\\n\")\n  for epoch in range(model_params[\"VALID_EPOCHS\"]):\n    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n  \n  console.save_text(os.path.join(output_dir,'logs.txt'))\n  \n  console.log(f\"[Validation Completed.]\\n\")\n  console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n  console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n  console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params={\n    \"MODEL\":\"facebook/bart-base\",#\"sshleifer/distilbart-cnn-6-6\",#\"sshleifer/distilbart-cnn-6-6\", #\"facebook/bart-base\",# model_type: facebook/bart-base\n    \"BATCH_SIZE_TRAIN\":8,          # training batch size\n    \"BATCH_SIZE_VALID\":8,          # validation batch size\n    \"TRAIN_EPOCHS\":1,              # number of training epochs\n    \"VALID_EPOCHS\":2,                # number of validation epochs\n    \"LEARNING_RATE\":2e-5,          # learning rate\n    \"MAX_LENGTH_SOURCE_TEXT\":1024,  # max length of source text\n    \"MAX_LENGTH_TARGET_TEXT\":200,   # max length of target text\n    \"SEED\": 42                     # set seed for reproducibility \n\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start Training","metadata":{}},{"cell_type":"code","source":"BartTrainer(dataframe=df ,source_data=\"text\", target_data=\"summary\", model_params=model_params, output_dir=\"outputs\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir ./outputs/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FOR ANALYSIS AND TESTING","metadata":{}},{"cell_type":"markdown","source":"Load Model","metadata":{}},{"cell_type":"code","source":"tokenizer = BartTokenizer.from_pretrained(\"/kaggle/working/outputs/model_files\", do_lower_case=False)\nmodel = BartForConditionalGeneration.from_pretrained(\"/kaggle/working/outputs/model_files\")\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sumeval.metrics.rouge import RougeCalculator\n\nrouge = RougeCalculator(stopwords=True, lang=\"en\")\n\ndef rouge_calc(preds, targets):\n    rouge_1 = rouge.rouge_n(summary=preds,references=targets,n=1) \n    rouge_2 = rouge.rouge_n(summary=preds,references=targets,n=2)\n    rouge_l = rouge.rouge_l(summary=preds,references=targets)\n\n    return {\"Rouge_1\": rouge_1,\n            \"Rouge_2\": rouge_2,\n            \"Rouge_L\": rouge_l}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cross attention","metadata":{}},{"cell_type":"code","source":"from IPython.display import display, HTML\nimport matplotlib as mpl\nfrom matplotlib.colors import Normalize, rgb2hex\nimport pandas as pd\nfrom IPython.display import HTML\nimport tensorflow as tf\n\ndef get_max_attn(c_atten):\n    lst1 = []\n    for target,i in enumerate(c_atten):\n        lst2 = []\n        for ipword in range(512):\n            max_head = 0.0\n            for layer in range(6):\n                max_ = 0\n                for head in range(8):\n                    if(max_ < c_atten[target][layer][0][head][0][ipword].tolist()):\n                        max_ = c_atten[target][layer][0][head][0][ipword].tolist()\n                max_head += max_\n            avg = max_head/6\n            lst2.append(avg)\n        lst1.append(lst2)\n    return lst1\n\ndef predict(model,tokenizer,parameters,sent, device):\n    sent = \" \".join(sent.split())\n    \n    source = tokenizer.__call__(\n            [sent],\n            max_length=parameters[\"MAX_LENGTH_SOURCE_TEXT\"],\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n    \n    ids = source[\"input_ids\"]\n    mask = source[\"attention_mask\"]\n    \n    model.eval()\n    with torch.no_grad():\n        ids = ids.to(device, dtype = torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        \n        generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150, \n#               num_beams=2,\n              repetition_penalty=2.5,  # there is a research paper for this\n              #length_penalty=1.0,  # > 0 encourages to generate short sentences, < 0 to generate long sentences\n#               early_stopping=True,  # stops beam search when number of beams sentences are generated per batch\n              output_attentions=True,\n              return_dict_in_generate=True\n              )\n        \n        \n        preds = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        #print(preds)\n    c_atten = generated_ids[\"cross_attentions\"]\n    \n    return c_atten, generated_ids, ids\n\ndef predict_(model,tokenizer,parameters,sent, device):\n    sent = \" \".join(sent.split())\n    \n    source = tokenizer.__call__(\n            [sent],\n            max_length=parameters[\"MAX_LENGTH_SOURCE_TEXT\"],\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n    \n    ids = source[\"input_ids\"]\n    mask = source[\"attention_mask\"]\n    \n    model.eval()\n    with torch.no_grad():\n        ids = ids.to(device, dtype = torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        \n        generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150, \n              repetition_penalty=2.5,  # there is a research paper for this\n              output_attentions=True,\n              return_dict_in_generate=True\n              )\n        \n        \n        preds = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n#         print(preds)\n        enumerated_preds = tokenizer.convert_ids_to_tokens(generated_ids.sequences[0])\n        print(\"enumerated predictions in token format: \")\n        for i,token in enumerate(enumerated_preds):\n            print(i,\":\",enumerated_preds[i])\n    c_atten = generated_ids[\"cross_attentions\"]\n    \n    return c_atten, generated_ids, ids\n\n\ndef colorize(attrs, cmap='bwr'):\n\n    cmap_bound = tf.reduce_max(tf.abs(attrs))\n\n    norm = Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n\n    cmap = mpl.cm.get_cmap(cmap)\n    colors = list(map(lambda x: rgb2hex(cmap(norm(x))), attrs))\n\n    return colors\n\ndef  hlstr(string, color='white'):\n\n    return f\"<mark style=background-color:{color}>{string} </mark>\"\n\n\n\ndef color_(max_atten_per_ipword , input_tokens):\n\n    input_tokens = [x[1:] for x in input_tokens]\n    colors = colorize(max_atten_per_ipword)\n    colored_input=[]\n    display(HTML(\"\".join(list(map(hlstr, input_tokens, colors)))))\n   \n\ndef cross_atten(model,tokenizer,parameters,sent, device):\n    \n    c_atten, generated_ids, input_ids = predict(model,tokenizer,parameters,sent, device)\n    \n    target_input_attn = get_max_attn(c_atten)\n    \n    max_atten_per_ipword = []\n    for ipword in range(512):\n        max_ = 0.0\n        for target in range(len(target_input_attn)):\n            if(max_ <= target_input_attn[target][ipword]):\n                max_ = target_input_attn[target][ipword]\n        max_atten_per_ipword.append(max_)\n    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    input_tokens = [token for token in input_tokens if token != '<pad>']\n    \n    color_(max_atten_per_ipword , input_tokens)\n\ndef cross_atten_per_word(model,tokenizer,parameters,sent, device):\n    \n    c_atten, generated_ids, input_ids = predict_(model,tokenizer,parameters,sent, device)\n    tarid = (int)(input(\"Enter the input id of the target word to be analysed :\"))\n    target_input_attn = get_max_attn(c_atten)\n    \n    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    input_tokens = [token for token in input_tokens if token != '<pad>']\n    \n    color_(target_input_attn[tarid] , input_tokens)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the pre-trained best-checkpoint model\nfrom datasets import load_dataset\ndef find_cross_atten(model, device, tokenizer, model_params,i):\n\n    test= load_dataset('multi_news', split='test[:5%]')\n    \n    sent = test[i][\"document\"]  # taking an example sentence\n    sent = \"summarize: \" + sent\n    sent = \" \".join(sent.split())\n    \n    source = tokenizer.__call__(\n            [sent],\n            max_length=model_params[\"MAX_LENGTH_SOURCE_TEXT\"],\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n    \n    ids = source[\"input_ids\"]\n    mask = source[\"attention_mask\"]\n    \n    model.eval()\n    with torch.no_grad():\n        ids = ids.to(device, dtype = torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        \n        generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150, \n              repetition_penalty=2.5,  # there is a research paper for this\n              )\n        \n        preds = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        \n        print(\"Input dialogue is: \", test[i][\"summary\"])\n        print(\"###############################\")\n        print(\"Output summary is: \", preds)\n        c=rouge_calc(preds,test[i][\"summary\"])\n        print(\"###############################\")\n        cross_atten(model,tokenizer,model_params,test[\"document\"][i], device)\n        print(c)\n    \n    \n    \ncuda =  torch.cuda.is_available()\ndevice = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n\n\nfind_cross_atten(model, device, tokenizer, model_params,7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate Rouge scores on Test data","metadata":{}},{"cell_type":"code","source":"# load the pre-trained best-checkpoint model\nfrom datasets import load_dataset\ndef inference(model, device, tokenizer, model_params,test):\n#     dataset = load_dataset(\"multi_news\")\n#     test = dataset[\"test\"]\n    #test = test.remove_columns([\"id\"])\n#     test= load_dataset('multi_news', split='test[:5%]')\n    \n    sent = test[\"document\"]  # taking an example sentence\n    sent = \"summarize: \" + sent\n    sent = \" \".join(sent.split())\n    \n    source = tokenizer.__call__(\n            [sent],\n            max_length=model_params[\"MAX_LENGTH_SOURCE_TEXT\"],\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n    \n    ids = source[\"input_ids\"]\n    mask = source[\"attention_mask\"]\n    \n    model.eval()\n    with torch.no_grad():\n        ids = ids.to(device, dtype = torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        \n        generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150, \n              repetition_penalty=2.5,  # there is a research paper for this\n             )\n        \n        preds = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        \n        c=rouge_calc(preds,test[\"summary\"])\n        return c\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdata = {'text': test['document'],\n        'summary': test['summary']}\ndf_test = pd.DataFrame(testdata)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c=[inference(model, device, tokenizer, model_params,test[i]) for i in range(len(df_test))]\n# print(np.array(c).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r1= [x.get('Rouge_1') for x in c]\nr2= [x.get('Rouge_2') for x in c]\nrl= [x.get('Rouge_L') for x in c]\nprint(\"mean values are Rouge_1:\",np.mean(r1),\" Rouge_2:\",np.mean(r2),\" Rouge_L:\",np.mean(rl))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}