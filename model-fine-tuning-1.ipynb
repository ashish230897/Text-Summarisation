{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install sumeval\n!pip install py7zr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decription \n### Fine-Tune a T5 model on the samsum dataset using Pytorch and HugingFace.","metadata":{}},{"cell_type":"code","source":"import gc\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sumeval.metrics.rouge import RougeCalculator\n\nimport torch\nfrom transformers import AutoTokenizer\nimport transformers\nfrom transformers import AutoModelForSeq2SeqLM\n\nprint('Pytorch version: %s'  % torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:50:32.413530Z","iopub.execute_input":"2022-10-04T12:50:32.413875Z","iopub.status.idle":"2022-10-04T12:50:32.420912Z","shell.execute_reply.started":"2022-10-04T12:50:32.413842Z","shell.execute_reply":"2022-10-04T12:50:32.419945Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Pytorch version: 1.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"warnings.simplefilter('ignore')\npd.set_option('display.max_colwidth', 100)\ncuda =  torch.cuda.is_available()\ndevice = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:50:36.325523Z","iopub.execute_input":"2022-10-04T12:50:36.325898Z","iopub.status.idle":"2022-10-04T12:50:36.331772Z","shell.execute_reply.started":"2022-10-04T12:50:36.325866Z","shell.execute_reply":"2022-10-04T12:50:36.330788Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data","metadata":{"execution":{"iopub.status.busy":"2022-10-02T13:28:32.324877Z","iopub.execute_input":"2022-10-02T13:28:32.325478Z","iopub.status.idle":"2022-10-02T13:28:32.351848Z","shell.execute_reply.started":"2022-10-02T13:28:32.325361Z","shell.execute_reply":"2022-10-02T13:28:32.350830Z"}}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"samsum\")","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:50:39.225566Z","iopub.execute_input":"2022-10-04T12:50:39.225947Z","iopub.status.idle":"2022-10-04T12:50:40.138396Z","shell.execute_reply.started":"2022-10-04T12:50:39.225915Z","shell.execute_reply":"2022-10-04T12:50:40.137490Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d928c6b345ab4483a9c1975d95c823f5"}},"metadata":{}}]},{"cell_type":"code","source":"train = dataset[\"train\"]\nvalid = dataset[\"validation\"]\ntest = dataset[\"test\"]\n\ntrain = train.remove_columns([\"id\"])\nvalid = valid.remove_columns([\"id\"])\ntest = test.remove_columns([\"id\"])\n\nprint(len(train), len(valid), len(test))\nprint(\"dataset has features: \", train)\nprint(\"sample input and output is\")\nprint(train[0][\"dialogue\"])\nprint(train[0][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:50:44.826040Z","iopub.execute_input":"2022-10-04T12:50:44.826390Z","iopub.status.idle":"2022-10-04T12:50:44.839836Z","shell.execute_reply.started":"2022-10-04T12:50:44.826360Z","shell.execute_reply":"2022-10-04T12:50:44.838630Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"14732 818 819\ndataset has features:  Dataset({\n    features: ['dialogue', 'summary'],\n    num_rows: 14732\n})\nsample input and output is\nAmanda: I baked  cookies. Do you want some?\nJerry: Sure!\nAmanda: I'll bring you tomorrow :-)\nAmanda baked cookies and will bring Jerry some tomorrow.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Analysis","metadata":{}},{"cell_type":"code","source":"# analyze input and summary lengths\ndialogue_lengths = [len(text.split()) for text in train[\"dialogue\"]]\nsummary_lengths = [len(text.split()) for text in train[\"summary\"]]\nprint(\"average length of dialogue is\", sum(dialogue_lengths)/len(dialogue_lengths))\nprint(\"average length of summary is\", sum(summary_lengths)/len(summary_lengths))","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:50:47.225859Z","iopub.execute_input":"2022-10-04T12:50:47.226219Z","iopub.status.idle":"2022-10-04T12:50:47.367565Z","shell.execute_reply.started":"2022-10-04T12:50:47.226187Z","shell.execute_reply":"2022-10-04T12:50:47.366478Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"average length of dialogue is 93.7863833831116\naverage length of summary is 20.3174721694271\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"# t5-small = 60M parameters\n# t5-base = 220M parameters\n# t5-large = 770M parameters\n# t5-3b = 3 billion\n# t5-11b = 11 billion\n# Loading tokenizer of t5 model\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:56:18.656777Z","iopub.execute_input":"2022-10-04T12:56:18.657160Z","iopub.status.idle":"2022-10-04T12:56:21.721922Z","shell.execute_reply.started":"2022-10-04T12:56:18.657129Z","shell.execute_reply":"2022-10-04T12:56:21.720813Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# prompting the model to do summarisation\nprefix = \"summarize: \"\n\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n    model_inputs[\"input_ids\"] = model_inputs[\"input_ids\"]\n\n    labels = tokenizer(text=examples[\"summary\"], max_length=128, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:56:24.965492Z","iopub.execute_input":"2022-10-04T12:56:24.966697Z","iopub.status.idle":"2022-10-04T12:56:24.973044Z","shell.execute_reply.started":"2022-10-04T12:56:24.966650Z","shell.execute_reply":"2022-10-04T12:56:24.971896Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"tokenized_train = train.map(preprocess_function, batched=True)\ntokenized_valid = valid.map(preprocess_function, batched=True)\ntokenized_test = test.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:56:27.275681Z","iopub.execute_input":"2022-10-04T12:56:27.276039Z","iopub.status.idle":"2022-10-04T12:56:35.072953Z","shell.execute_reply.started":"2022-10-04T12:56:27.276007Z","shell.execute_reply":"2022-10-04T12:56:35.072047Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38742aaec6342a2a236ca76933afb84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13572ef737b64fda89beb49714417339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d735dbda27c74ab69f364a629309d3a7"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_train = tokenized_train.remove_columns([\"dialogue\"]).remove_columns([\"summary\"]).remove_columns([\"attention_mask\"])\ntokenized_valid = tokenized_valid.remove_columns([\"dialogue\"]).remove_columns([\"summary\"]).remove_columns([\"attention_mask\"])\ntokenized_test = tokenized_test.remove_columns([\"dialogue\"]).remove_columns([\"summary\"]).remove_columns([\"attention_mask\"])\n\ntokenized_train.set_format(\"torch\")\ntokenized_valid.set_format(\"torch\")\ntokenized_test.set_format(\"torch\")\n\nprint(\"data format now is\", tokenized_train)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:56:38.605748Z","iopub.execute_input":"2022-10-04T12:56:38.606106Z","iopub.status.idle":"2022-10-04T12:56:38.631073Z","shell.execute_reply.started":"2022-10-04T12:56:38.606076Z","shell.execute_reply":"2022-10-04T12:56:38.630107Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"data format now is Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 14732\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading the model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")  # has 60M parameters","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:56:45.886239Z","iopub.execute_input":"2022-10-04T12:56:45.886619Z","iopub.status.idle":"2022-10-04T12:56:47.330447Z","shell.execute_reply.started":"2022-10-04T12:56:45.886585Z","shell.execute_reply":"2022-10-04T12:56:47.329434Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data.dataloader import default_collate\n\ndef pad_collate(batch):\n    xx = [x[\"input_ids\"] for x in batch]\n    yy = [x[\"labels\"] for x in batch]\n    x_lens = [len(x) for x in xx]\n    y_lens = [len(y) for y in yy]\n\n    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n    yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n    \n    data = [{\"input_ids\": x, \"labels\": y} for x,y in zip(xx_pad, yy_pad)]\n    \n    return default_collate(data)\n\ntrain_dataloader = DataLoader(tokenized_train, shuffle=True, batch_size=8, collate_fn=pad_collate)\neval_dataloader = DataLoader(tokenized_valid, batch_size=8, collate_fn=pad_collate)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:57:29.925647Z","iopub.execute_input":"2022-10-04T12:57:29.926009Z","iopub.status.idle":"2022-10-04T12:57:29.935644Z","shell.execute_reply.started":"2022-10-04T12:57:29.925977Z","shell.execute_reply":"2022-10-04T12:57:29.934637Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"from transformers import get_scheduler\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 4\nnum_training_steps = num_epochs * len(train_dataloader)\nprint(\"Training steps are\", num_training_steps)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:52:23.685592Z","iopub.execute_input":"2022-10-04T12:52:23.686261Z","iopub.status.idle":"2022-10-04T12:52:23.703657Z","shell.execute_reply.started":"2022-10-04T12:52:23.686224Z","shell.execute_reply":"2022-10-04T12:52:23.702459Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Training steps are 7368\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge = RougeCalculator(stopwords=True, lang=\"en\")\n\ndef rouge_calc(preds, targets):\n    rouge_1 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=1) for i in range(len(preds))]\n    rouge_2 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=2) for i in range(len(preds))]\n    rouge_l = [rouge.rouge_l(summary=preds[i],references=targets[i]) for i in range(len(preds))]\n\n    return {\"Rouge_1\": np.array(rouge_1).mean(),\n            \"Rouge_2\": np.array(rouge_2).mean(),\n            \"Rouge_L\": np.array(rouge_l).mean()}\n\ndef evaluate(model, eval_dataloader, tokenizer):\n    prediction = []\n    ground_truth = []\n    losses = []\n    for eval_batch in eval_dataloader:\n        eval_batch = {k: v.to(device) for k, v in eval_batch.items()}\n        outputs = model.generate(eval_batch['input_ids'])\n        losses.append(model(**eval_batch).loss.item())\n        for i in range(0,len(eval_batch)):\n            prediction.append(tokenizer.decode(outputs[i], skip_special_tokens=True))\n            ground_truth.append(tokenizer.decode(eval_batch['labels'][i], skip_special_tokens=True))\n\n    scores = rouge_calc(prediction , ground_truth)\n    avg_loss = sum(losses)/len(losses)\n    print(\"Validation data score and losses are\", scores, avg_loss)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-04T13:31:50.465831Z","iopub.execute_input":"2022-10-04T13:31:50.466211Z","iopub.status.idle":"2022-10-04T13:31:50.478970Z","shell.execute_reply.started":"2022-10-04T13:31:50.466178Z","shell.execute_reply":"2022-10-04T13:31:50.477480Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.to(device)\nmodel.train()\n\nstep = 0\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        \n        # evaluate on every 100th step\n        if step % 100 == 0:\n            print(\"Train loss on {}th step is {}\".format(step, loss.item()))\n            evaluate(model, eval_dataloader, tokenizer)\n        step += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make some predictions","metadata":{}},{"cell_type":"code","source":"batch = next(iter(eval_dataloader))\nbatch = {k: v.to(device) for k, v in batch.items()}\noutputs = model.generate(batch['input_ids'])\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nprint(tokenizer.decode(batch['labels'][0], skip_special_tokens=True))\nprint(tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-04T12:29:18.225627Z","iopub.execute_input":"2022-10-04T12:29:18.225985Z","iopub.status.idle":"2022-10-04T12:29:18.416249Z","shell.execute_reply.started":"2022-10-04T12:29:18.225955Z","shell.execute_reply":"2022-10-04T12:29:18.415179Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"A wants to get a puppy for her son. He will take him to the animal shelter\nA will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.\nsummarize: A: Hi Tom, are you busy tomorrow’s afternoon? B: I’m pretty sure I am. What’s up? A: Can you go with me to the animal shelter?. B: What do you want to do? A: I want to get a puppy for my son. B: That will make him so happy. A: Yeah, we’ve discussed it many times. I think he’s ready now. B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) A: I'll get him one of those little dogs. B: One that won't grow up too big;-) A: And eat too much;-)) B: Do you know which one he would like? A: Oh, yes, I took him there last Monday. He showed me one that he really liked. B: I bet you had to drag him away. A: He wanted to take it home right away ;-). B: I wonder what he'll name it. A: He said he’d name it after his dead hamster – Lemmy - he's a great Motorhead fan :-)))\n","output_type":"stream"}]}]}